{
	"paper_1": "Abstract\n001 Large Language Model (LLM) agents signif002 icantly extend the capabilities of standalone\n003 LLMs, empowering them to interact with exter004 nal tools (e.g., APIs, functions) and complete\n005 various tasks in a self-directed fashion. The\n006 challenge of tool use demands that LLMs not\n007 only understand user queries and generate an008 swers accurately but also excel in task plan009 ning, tool invocation, and result summariza010 tion. While traditional works focus on train011 ing a single LLM with all these capabilities,\n012 performance limitations become apparent, par013 ticularly with smaller models. To overcome\n014 these challenges, we propose a novel approach\n015 that decomposes the aforementioned capabili016 ties into a planner, caller, and summarizer. Each\n017 component is implemented by a single LLM\n018 that focuses on a specific capability and collab019 orates with others to accomplish the task. This\n020 modular framework facilitates individual up021 dates and the potential use of smaller LLMs for\n022 building each capability. To effectively train\n023 this framework, we introduce a two-stage train024 ing paradigm. First, we fine-tune a backbone\n025 LLM on the entire dataset without discrimi026 nating sub-tasks, providing the model with a\n027 comprehensive understanding of the task. Sec028 ond, the fine-tuned LLM is used to instanti029 ate the planner, caller, and summarizer respec030 tively, which are continually fine-tuned on re031 spective sub-tasks. Evaluation across various\n032 tool-use benchmarks illustrates that our pro033 posed multi-LLM framework surpasses the tra034 ditional single-LLM approach, highlighting its\n035 efficacy and advantages in tool learning.",
	"paper_2": "Abstract\n001 Adapting Large Language Models (LLMs) for\n002 agent tasks is critical in developing language\n003 agents. Direct Preference Optimization (DPO)\n004 is a promising technique for this adaptation\n005 with the alleviation of compounding errors, of006 fering a means to directly optimize Reinforce007 ment Learning (RL) objectives. However, ap008 plying DPO to multi-turn tasks presents chal009 lenges due to the inability to cancel the par010 tition function. Overcoming this obstacle in011 volves making the partition function indepen012 dent of the current state and addressing length\n013 disparities between preferred and dis-preferred\n014 trajectories. In this light, we replace the pol015 icy constraint with the state-action occupancy\n016 measure constraint in the RL objective and\n017 add length normalization to the Bradley-Terry\n018 model, yielding a novel loss function named\n019 DMPO for multi-turn agent tasks with theoret020 ical explanations. Extensive experiments on\n021 three multi-turn agent task datasets confirm the\n022 effectiveness and superiority of the DMPO loss.",
	"paper_3": "Abstract\n001 Multimodal models leverage large-scale pre002 training to achieve strong but still imperfect\n003 performance on tasks such as image captioning,\n004 visual question answering, and cross-modal re005 trieval. In this paper, we present a simple and\n006 efficient method for correcting errors in trained\n007 contrastive image-text retrieval models with no\n008 additional training, called Nearest Neighbor\n009 Normalization (NNN). We show an improvement\n010 on retrieval metrics in both text retrieval and\n011 image retrieval for all of the contrastive models\n012 that we tested (CLIP, BLIP, ALBEF, SigLIP,\n013 BEiT) and for both of the datasets that we used\n014 (MS-COCO and Flickr30k). NNN requires a ref015 erence database, but does not require any train016 ing on this database, and can even increase the\n017 retrieval accuracy of a model after finetuning.",
	"paper_4": "Abstract\n001 Ensuring the security of released large language\n002 models (LLMs) poses a significant dilemma, as\n003 existing mechanisms either compromise own004 ership rights or raise data privacy concerns.\n005 To address this dilemma, we introduce Tay006 lorMLP to protect the ownership of released\n007 LLMs and prevent their abuse. Specifically,\n008 TaylorMLP preserves the ownership of LLMs\n009 by transforming the weights of LLMs into pa010 rameters of Taylor-series. Instead of releas011 ing the original weights, developers can re012 lease the Taylor-series parameters with users,\n013 thereby ensuring the security of LLMs. More014 over, TaylorMLP can prevent abuse of LLMs\n015 by adjusting the generation speed. It can induce\n016 low-speed token generation for the protected\n017 LLMs by increasing the terms in the Taylor018 series. This intentional delay helps LLM de019 velopers prevent potential large-scale unautho020 rized uses of their models. Empirical experi021 ments across five datasets and three LLM archi022 tectures demonstrate that TaylorMLP induces\n023 over 4Ã— increase in latency, producing the to024 kens precisely matched with original LLMs.\n025 Subsequent defensive experiments further con026 firm that TaylorMLP effectively prevents users\n027 from reconstructing the weight values based\n028 on downstream datasets. The source code\n029 is available at https://anonymous.4open.\n030 science/r/TaylorMLP-6EBE.",
	"paper_5": "Abstract\n001 Tool learning aims to enhance and expand large\n002 language models' (LLMs) capabilities with ex003 ternal tools, which has gained significant atten004 tion recently. Current methods have shown that\n005 LLMs can effectively handle a certain amount\n006 of tools through in-context learning or fine007 tuning. However, in real-world scenarios, the\n008 number of tools is typically extensive and ir009 regularly updated, emphasizing the necessity\n010 for a dedicated tool retrieval component. Tool\n011 retrieval is nontrivial due to the following chal012 lenges: 1) complex user instructions and tool\n013 descriptions; 2) misalignment between tool re014 trieval and tool usage models. To address the\n015 above issues, we propose to enhance tool re016 trieval with iterative feedback from the large\n017 language model. Specifically, we prompt the\n018 tool usage model, i.e., the LLM, to provide\n019 feedback for the tool retriever model in multi020 round, which could progressively improve the\n021 tool retriever's understanding of instructions\n022 and tools and reduce the gap between the two\n023 standalone components. We build a unified\n024 and comprehensive benchmark to evaluate tool\n025 retrieval models. The extensive experiments\n026 indicate that our proposed approach achieves\n027 advanced performance in both in-domain eval028 uation and out-of-domain evaluation.",
	"paper_6": "Abstract\n001 Although most current large multimodal mod002 els (LMMs) can already understand photos of\n003 natural scenes and portraits, their understand004 ing of abstract images, e.g., charts, maps, or\n005 layouts, and visual reasoning capabilities re006 mains quite rudimentary. They often struggle\n007 with simple daily tasks, such as reading time\n008 from a clock, understanding a flowchart, or\n009 planning a route using a road map. In light\n010 of this, we design a multi-modal self-instruct,\n011 utilizing large language models and their code\n012 capabilities to synthesize massive abstract im013 ages and visual reasoning instructions across\n014 daily scenarios. Our strategy effortlessly cre015 ates a multimodal benchmark with 11,193 in016 structions for eight visual scenarios: charts, ta017 bles, simulated maps, dashboards, flowcharts,\n018 relation graphs, floor plans, and visual puzzles.\n019 This benchmark, constructed with simple\n020 lines and geometric elements, exposes the\n021 shortcomings of most advanced LMMs like\n022 GPT-4V and Llava in abstract image under023 standing, spatial relations reasoning, and visual\n024 element induction. Besides, to verify the qual025 ity of our synthetic data, we fine-tune an LMM\n026 using 62,476 synthetic chart, table and road\n027 map instructions. The results demonstrate im028 proved chart understanding and map navigation\n029 performance, and also demonstrate potential\n030 benefits for other visual reasoning tasks. Our\n031 code and data are available at this anonymous\n032 link: https://anonymous.4open.science/\n033 r/self-instruct-data-engine-E785/"
}
